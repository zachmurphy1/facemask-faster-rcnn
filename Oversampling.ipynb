{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oversampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO6Tuoot4JL2LBcYWHnL/ht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zachmurphy1/facemask-faster-rcnn/blob/main/Oversampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQlLrR-0F_GD"
      },
      "source": [
        "# Oversampling\n",
        "This notebook performs oversampling on the train data.\n",
        "\n",
        "## Input\n",
        "Images and annotations in the train directory.\n",
        "```\n",
        "facemask_data/images/train/images\n",
        "facemask_data/images/train/annotations\n",
        "```\n",
        "\n",
        "## Output\n",
        "Oversampled images and annotations\n",
        "```\n",
        "facemask_data/images/train/oversampling/images\n",
        "facemask_data/images/train/oversampling/annotations\n",
        "```\n",
        "\n",
        "Instance counts by class pre-oversampling\n",
        "```\n",
        "facemask_data/train/oversampling/train_instance_counts.pkl\n",
        "```\n",
        "\n",
        "(Intermediate) Instance crops with annotations by class\n",
        "```\n",
        "facemask_data/images/train/oversampling/no_mask/images\n",
        "facemask_data/images/train/oversampling/no_mask/annotations\n",
        "\n",
        "facemask_data/images/train/oversampling/masked/images\n",
        "facemask_data/images/train/oversampling/masked/annotations\n",
        "\n",
        "facemask_data/images/train/oversampling/incorrect/images\n",
        "facemask_data/images/train/oversampling/incorrect/annotations\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I1tmGIMxP_s"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys, os, shutil\n",
        "\n",
        "from PIL import Image\n",
        "from bs4 import BeautifulSoup\n",
        "import torch, torchvision\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV5SjU8Vx7AU",
        "outputId": "db55ea01-dff3-4238-ca72-8432c05429e3"
      },
      "source": [
        "# Mount data directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd /content/gdrive/My\\ Drive/facemask-faster-rcnn/\n",
        "\n",
        "DATADIR = 'facemask_data/train'\n",
        "ANNDIR = DATADIR + '/annotations'\n",
        "IMGDIR = DATADIR + '/images'\n",
        "\n",
        "n = len(next(os.walk(IMGDIR))[2])\n",
        "print('# imgs:',n)\n",
        "target_dir = DATADIR + '/' + 'oversampling'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/DL Final Project\n",
            "# imgs: 511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7cJphkDOkpZ"
      },
      "source": [
        "## Get instance crops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcuLK2OzIQVu"
      },
      "source": [
        "# Make dirs\n",
        "os.makedirs(DATADIR + '/oversampling', exist_ok=True)\n",
        "for f in ['no_mask','masked','incorrect']:\n",
        "  os.makedirs(target_dir + '/' + f, exist_ok=True)\n",
        "  os.makedirs(target_dir + '/' + f +'/images', exist_ok=True)\n",
        "  os.makedirs(target_dir + '/' + f +'/annotations', exist_ok=True)\n",
        "\n",
        "# Set bounding box border\n",
        "border = 10\n",
        "\n",
        "# For each image\n",
        "counts = {'no_mask':0, 'masked':0, 'incorrect':0}\n",
        "for i in range(n):\n",
        "  # Get image\n",
        "  img = Image.open(IMGDIR + '/' + str(i) + '.png').convert('RGB')\n",
        "\n",
        "  # Get annotations\n",
        "  ann_path = ANNDIR + '/' + str(i) + '.xml'\n",
        "  with open(ann_path) as f:\n",
        "    ann_xml = f.read()\n",
        "  ann_parsed = BeautifulSoup(ann_xml,'xml')\n",
        "  objects = ann_parsed.find_all('object')\n",
        "  n_objs = len(objects)\n",
        "\n",
        "  # Get image shape\n",
        "  width = int(ann_parsed.find('width').text)\n",
        "  height = int(ann_parsed.find('height').text)\n",
        "\n",
        "  for o in objects:\n",
        "    # Get bbox\n",
        "    xmin = int(o.find('xmin').text)\n",
        "    ymin = int(o.find('ymin').text)\n",
        "    xmax = int(o.find('xmax').text)\n",
        "    ymax = int(o.find('ymax').text)\n",
        "\n",
        "    # Get image boundaries\n",
        "    left_border = min(border,xmin)\n",
        "    top_border = min(border,ymin)\n",
        "    right_border = min(border,width-xmax)\n",
        "    bottom_border = min(border,height-ymax)\n",
        "\n",
        "    # Crop to instance\n",
        "    cropped = img.copy()\n",
        "    cropped = cropped.crop((xmin-left_border,ymin-top_border,xmax+right_border,ymax+bottom_border))\n",
        "\n",
        "    # Edit bbox\n",
        "    o.find('xmin').string.replaceWith(str(left_border))\n",
        "    o.find('ymin').string.replaceWith(str(top_border))\n",
        "    o.find('xmax').string.replaceWith(str(left_border+xmax-xmin))\n",
        "    o.find('ymax').string.replaceWith(str(top_border+ymax-ymin))\n",
        "    \n",
        "    # Get target path\n",
        "    mask_class = o.find('name').text\n",
        "    prefix = ''\n",
        "    if mask_class == 'without_mask':\n",
        "      dir = target_dir + '/no_mask'\n",
        "      prefix = 'no_mask'\n",
        "    elif mask_class == 'with_mask':\n",
        "      dir = target_dir + '/masked'\n",
        "      prefix = 'masked'\n",
        "    elif mask_class == 'mask_weared_incorrect':\n",
        "      dir = target_dir + '/incorrect'\n",
        "      prefix = 'incorrect'\n",
        "    else:\n",
        "      print('mask label error')\n",
        "\n",
        "    # Save image\n",
        "    cropped.save(dir + '/images/' + str(counts[prefix]) + '.png', 'PNG')\n",
        "\n",
        "    # Save annotation\n",
        "    with open (dir + '/annotations/' + str(counts[prefix]) + '.xml', 'w') as f:\n",
        "      f.write(o.prettify())\n",
        "      f.close()\n",
        "\n",
        "    # Increment counts\n",
        "    counts[prefix] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1PRFivfIKaG"
      },
      "source": [
        "{'no_mask': 461, 'masked': 1862, 'incorrect': 66}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g70xk2KI2t02"
      },
      "source": [
        "with open(target_dir + '/train_instance_counts.pkl', 'wb') as f:\n",
        "  pickle.dump(counts,f)\n",
        "print(counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw0GW57lOoa3"
      },
      "source": [
        "## Stitch instances into images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "if-Ypmw3Xn7t",
        "outputId": "00e3a184-7f67-4d61-b4fc-2e8fdd902452"
      },
      "source": [
        "# Copy original images\n",
        "orig_img_dir = '/content/gdrive/MyDrive/DL Final Project/facemask_data/train/oversampling/images'\n",
        "orig_ann_dir = '/content/gdrive/MyDrive/DL Final Project/facemask_data/train/oversampling/annotations'\n",
        "shutil.copytree('/content/gdrive/MyDrive/DL Final Project/facemask_data/train/images',orig_img_dir)\n",
        "shutil.copytree('/content/gdrive/MyDrive/DL Final Project/facemask_data/train/annotations',orig_ann_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/DL Final Project/facemask_data/train/oversampling/annotations'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3RTbpwlOdzX"
      },
      "source": [
        "for mask_class in ['incorrect','no_mask', 'masked']:\n",
        "  # Get number to augment\n",
        "  n_to_augment = int((max(v for k,v in counts.items()) - counts[mask_class])*3/4)\n",
        "\n",
        "  # Set dirs\n",
        "  paste_img_dir = target_dir + '/' + mask_class +'/images'\n",
        "  paste_ann_dir = target_dir + '/' + mask_class +'/annotations'\n",
        "  _, _, paste_paths = next(os.walk(paste_img_dir))\n",
        "  _, _, orig_paths =  next(os.walk(orig_img_dir))\n",
        "\n",
        "  # For each augmentation\n",
        "  for n in range(n_to_augment):\n",
        "    # Randomly select image to paste\n",
        "    paste_idx = np.random.choice(paste_paths)[:-4]\n",
        "    paste_img = Image.open(paste_img_dir + '/' + paste_idx + '.png')\n",
        "    with open(paste_ann_dir + '/' + paste_idx + '.xml') as f:\n",
        "      ann_xml = f.read()\n",
        "      f.close()\n",
        "    paste_ann = BeautifulSoup(ann_xml,'xml')\n",
        "\n",
        "    # Try until get a match\n",
        "    passed = False\n",
        "    while passed == False:\n",
        "      passed = True\n",
        "      # Randomly select background image\n",
        "      orig_idx = np.random.choice(orig_paths)[:-4]\n",
        "      try:\n",
        "        orig_img = Image.open(orig_img_dir + '/' + orig_idx + '.png').convert('rgb')\n",
        "      except:\n",
        "        orig_img = Image.open(orig_img_dir + '/' + orig_idx + '.png')\n",
        "      with open(orig_ann_dir + '/' + orig_idx + '.xml') as f:\n",
        "        ann_xml = f.read()\n",
        "        f.close()\n",
        "      orig_ann = BeautifulSoup(ann_xml,'lxml')\n",
        "\n",
        "      # Get background ann data\n",
        "      orig_width = int(orig_ann.find('width').text)\n",
        "      orig_height = int(orig_ann.find('height').text)\n",
        "\n",
        "      orig_o = orig_ann.find_all('object')\n",
        "      orig_bbs = []\n",
        "      for o in orig_o:\n",
        "        orig_bbs.append([int(o.find('xmin').text),\n",
        "                         int(o.find('ymin').text),\n",
        "                         int(o.find('xmax').text),\n",
        "                         int(o.find('ymax').text)])\n",
        "\n",
        "      # Get paste box\n",
        "      paste_bb = [round(float(paste_ann.find('xmin').text)),\n",
        "                  round(float(paste_ann.find('ymin').text)),\n",
        "                  round(float(paste_ann.find('xmax').text)),\n",
        "                  round(float(paste_ann.find('ymax').text))]\n",
        "\n",
        "      xshift = paste_bb[0]\n",
        "      yshift = paste_bb[1]\n",
        "      xwidth = paste_bb[2] - paste_bb[0]\n",
        "      ywidth = paste_bb[3] - paste_bb[1]\n",
        "      \n",
        "      paste_img_w = paste_img.width\n",
        "      paste_img_h = paste_img.height\n",
        "\n",
        "      # Scale paste to mean of background bounding box widths but within 0.25-4\n",
        "      orig_w_mean = np.mean([x[2] - x[0] for x in orig_bbs])\n",
        "      scale = orig_w_mean / xwidth\n",
        "      scale = scale * np.random.uniform(0.8,1.2)\n",
        "      if scale > 4 or scale < 0.25:\n",
        "        passed = False\n",
        "        continue\n",
        "\n",
        "      # Get scaled paste image size\n",
        "      paste_img_w = int(round(paste_img_w*scale))\n",
        "      paste_img_h = int(round(paste_img_h*scale))\n",
        "\n",
        "      # Scale paste bounding box\n",
        "      paste_img_scaled = paste_img.resize((paste_img_w, paste_img_h))\n",
        "      paste_bb = [int(x*scale) for x in paste_bb]\n",
        "      x_border = paste_bb[0]\n",
        "      y_border = paste_bb[1]\n",
        "      paste_width = paste_bb[2] - paste_bb[0]\n",
        "      paste_height = paste_bb[3] - paste_bb[1]\n",
        "\n",
        "      # Random horizontal flip\n",
        "      if np.random.choice([True, False]):\n",
        "        paste_img_scaled = paste_img_scaled.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        paste_bb[0] = paste_img_scaled.width - paste_bb[2]\n",
        "        paste_bb[2] = paste_img_scaled.width - x_border\n",
        "      \n",
        "      # Random position\n",
        "      try:\n",
        "        pos_x = np.random.randint(0,orig_img.width-paste_img_w)\n",
        "        pos_y = np.random.randint(0,orig_img.height-paste_img_h)\n",
        "      except:\n",
        "        passed = False\n",
        "        continue\n",
        "\n",
        "      # Get paste box in terms of background image coords\n",
        "      paste_bb[0] += pos_x\n",
        "      paste_bb[1] += pos_y\n",
        "      paste_bb[2] = paste_bb[0] + paste_width\n",
        "      paste_bb[3] = paste_bb[1] + paste_height\n",
        "\n",
        "      # IOU check\n",
        "      iou_threshold = 0\n",
        "      border_threshold = int(orig_img.width/20)\n",
        "      for o in orig_bbs:\n",
        "        ot = torch.Tensor([o[0],\n",
        "                           o[1],\n",
        "                           o[2],\n",
        "                           o[3]]).unsqueeze(0)\n",
        "        pt = torch.Tensor([pos_x-border_threshold,\n",
        "                           pos_y - border_threshold,\n",
        "                           pos_x + paste_img_scaled.width + border_threshold,\n",
        "                           pos_y + paste_img_scaled.height + border_threshold]).unsqueeze(0)\n",
        "\n",
        "        iou = torchvision.ops.box_iou(ot,pt)\n",
        "        if iou > iou_threshold:\n",
        "          passed = False\n",
        "    \n",
        "    # If passed, commit\n",
        "    orig_img.paste(paste_img_scaled,(pos_x,pos_y))\n",
        "\n",
        "    # Save image\n",
        "    orig_img.save(orig_img_dir + '/' + orig_idx + '.png', 'PNG')\n",
        "\n",
        "    # Save annotation\n",
        "    paste_ann.find('xmin').string.replaceWith(str(paste_bb[0]))\n",
        "    paste_ann.find('ymin').string.replaceWith(str(paste_bb[1]))\n",
        "    paste_ann.find('xmax').string.replaceWith(str(paste_bb[2]))\n",
        "    paste_ann.find('ymax').string.replaceWith(str(paste_bb[3]))\n",
        "    orig_ann.find('annotation').append(paste_ann)\n",
        "\n",
        "    with open(orig_ann_dir + '/' + orig_idx + '.xml', 'w') as f:\n",
        "      f.write(orig_ann.prettify())\n",
        "      f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}