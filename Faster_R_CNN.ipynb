{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Faster R-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zachmurphy1/facemask-faster-rcnn/blob/main/Faster_R_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ajdj3xFc7DB"
      },
      "source": [
        "# Faster R-CNN\n",
        "This notebook implements and trains the Faster R-CNN network.\n",
        "\n",
        "## Input\n",
        "Images and annotations from train, train (OS), val, and test sets\n",
        "```\n",
        "Train:\n",
        "facemask_data/train/images\n",
        "facemask_data/train/annotations\n",
        "\n",
        "Train (OS):\n",
        "facemask_data/train/oversampling/images\n",
        "facemask_data/train/oversampling/annotations\n",
        "\n",
        "Val:\n",
        "facemask_data/val/images\n",
        "facemask_data/val/annotations\n",
        "\n",
        "Test:\n",
        "facemask_data/test/images\n",
        "facemask_data/test/annotations\n",
        "```\n",
        "\n",
        "## Output\n",
        "Training class instance\n",
        "```\n",
        "Models/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBpU0bPydN6F"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJMvim8U9Sqw"
      },
      "source": [
        "# Imports\n",
        "import pickle\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from bs4 import BeautifulSoup\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsxUZ16W69kz"
      },
      "source": [
        "# Mount data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2kSvDpz2GEZ",
        "outputId": "334c7006-a900-4eae-d97f-8469aed39639"
      },
      "source": [
        "# Mount data directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd /content/gdrive/My\\ Drive/facemask-faster-rcnn/\n",
        "\n",
        "DATADIR = 'facemask_data'\n",
        "ANNDIR = DATADIR + '/annotations'\n",
        "IMGDIR = DATADIR + '/images'\n",
        "\n",
        "SRPATH = 'sr_training/sr_model.pkl'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/DL Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135QnWMdf7ii"
      },
      "source": [
        "## SR network architecture\n",
        "Needed for importing SR network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8Ovua7de2-X"
      },
      "source": [
        "# Set upscaling factor\n",
        "sr_scale=4\n",
        "\n",
        "class Bblock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Bblock,self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(64,64,(3,3),stride=1,padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.prelu = nn.PReLU(64)\n",
        "    self.conv2 = nn.Conv2d(64,64,(3,3),stride=1,padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    skip = x\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.prelu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = out + skip\n",
        "    return out\n",
        "\n",
        "\n",
        "class Upscale(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Upscale,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(64,256,(3,3),stride=1,padding=1)\n",
        "    self.pixelShuffle = nn.PixelShuffle(2)\n",
        "    self.prelu = nn.PReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.pixelShuffle(out)\n",
        "    out = self.prelu(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "class SRNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SRNetwork,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3,64,(9,9),stride=1,padding=4)\n",
        "    self.prelu = nn.PReLU()\n",
        "\n",
        "    bres_modules = []\n",
        "    for i in range(16):\n",
        "      bres_modules.append(Bblock())\n",
        "    self.Bres = nn.Sequential(*bres_modules)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(64,64,(3,3),stride=1,padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "    self.upscale1 = Upscale()\n",
        "    self.upscale2 = Upscale()\n",
        "\n",
        "    self.conv3 = nn.Conv2d(64,3,(9,9),stride=1,padding=4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.prelu(out)\n",
        "    skip = out\n",
        "    out = self.Bres(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    out = out + skip\n",
        "\n",
        "    out = self.upscale1(out)\n",
        "    out = self.upscale2(out)\n",
        "\n",
        "    out = self.conv3(out)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h6Ocriw9pZO"
      },
      "source": [
        "# Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucS9U1NR9tzz"
      },
      "source": [
        "class MaskDataset(Dataset):\n",
        "  def __init__(self, op='train', sr=False, do_transforms=True):\n",
        "    # Set data dir based on op\n",
        "    self.op = op\n",
        "    if self.op in ['train', 'val', 'test', 'train/oversampling']:\n",
        "      # Get data dirs and metadata\n",
        "      self.data_dir = DATADIR + '/' + self.op\n",
        "      self.ann_dir = os.path.join(self.data_dir,'annotations')\n",
        "      self.img_dir = os.path.join(self.data_dir,'images')\n",
        "\n",
        "      self.files = next(os.walk(self.img_dir))[2]\n",
        "      self.n = len(self.files)\n",
        "      self.do_transforms=do_transforms\n",
        "\n",
        "      # Get instance counts by class\n",
        "      # For each image\n",
        "      counts = {'no_mask':0,\n",
        "                'masked':0,\n",
        "                'incorrect':0}\n",
        "      for i in range(self.n):\n",
        "        # Get annotations\n",
        "        ann_path = self.ann_dir + '/' + str(i) + '.xml'\n",
        "        with open(ann_path, 'r') as f:\n",
        "          ann_xml = f.read()\n",
        "        ann_parsed = BeautifulSoup(ann_xml,'lxml')\n",
        "        objects = ann_parsed.find_all('object')\n",
        "        n_objs = len(objects)\n",
        "\n",
        "        for o in objects:\n",
        "          # Get target path\n",
        "          mask_class = o.find('name').text.strip()\n",
        "          prefix = ''\n",
        "          if mask_class == 'without_mask':\n",
        "            prefix = 'no_mask'\n",
        "          elif mask_class == 'with_mask':\n",
        "            prefix = 'masked'\n",
        "          elif mask_class == 'mask_weared_incorrect':\n",
        "            prefix = 'incorrect'\n",
        "          else:\n",
        "            print('mask label error')\n",
        "          # Increment count\n",
        "          counts[prefix] += 1\n",
        "      self.counts = counts\n",
        "      \n",
        "    else:\n",
        "      print('op should be train, val, or test')\n",
        "\n",
        "    # Set SR conditions\n",
        "    self.sr = sr\n",
        "    if self.sr:\n",
        "      with open(SRPATH, 'rb') as f:\n",
        "        self.sr_model = pickle.load(f)\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Get image\n",
        "    img_path = self.img_dir + '/' + self.files[idx][:-4] + '.png'\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    # Get annotations\n",
        "    ann_path = self.ann_dir + '/' + self.files[idx][:-4]  + '.xml'\n",
        "    with open(ann_path) as f:\n",
        "      ann_xml = f.read()\n",
        "      ann_parsed = BeautifulSoup(ann_xml,'xml')\n",
        "      objects = ann_parsed.find_all('object')\n",
        "    n_objs = len(objects)\n",
        "\n",
        "    # Get ground truth bboxes and labels\n",
        "    def getBbox(obj):\n",
        "      return [int(o.find('xmin').text),int(o.find('ymin').text),int(o.find('xmax').text),int(o.find('ymax').text)]\n",
        "    \n",
        "    def getLabel(obj):\n",
        "      label = obj.find('name').text.strip()\n",
        "      if label == 'without_mask':\n",
        "        return 1\n",
        "      elif label == 'with_mask':\n",
        "        return 2\n",
        "      elif label == 'mask_weared_incorrect':\n",
        "        return 3\n",
        "      else:\n",
        "        raise Exception(\"Unknown label '{}'\".format(label))\n",
        "\n",
        "    def getSize(obj):\n",
        "      size_xml = obj.parent.find('size')\n",
        "      width = int(size_xml.find('width').text)\n",
        "      height = int(size_xml.find('height').text)\n",
        "      return [width,height]\n",
        "    \n",
        "    bboxes = []\n",
        "    labels = []\n",
        "    size = []\n",
        "    for o in objects:\n",
        "      bboxes.append(getBbox(o))\n",
        "      labels.append(getLabel(o))\n",
        "      size.append(getSize(o))\n",
        "    \n",
        "    # Combine annotations into tensor dict\n",
        "    ann = {\n",
        "        'boxes': torch.as_tensor(bboxes),\n",
        "        'labels': torch.as_tensor(labels),\n",
        "        'image_id': torch.as_tensor([idx]),\n",
        "        'size': torch.as_tensor(size)\n",
        "    }\n",
        "\n",
        "    # To tensor\n",
        "    to_tensor = transforms.ToTensor()\n",
        "    img = to_tensor(img)\n",
        "\n",
        "    # SR\n",
        "    if self.sr:\n",
        "      with torch.no_grad():\n",
        "        if torch.cuda.is_available():\n",
        "          img = img.cuda()\n",
        "        img = self.sr_model(img.unsqueeze(0))\n",
        "        img = img[0]\n",
        "        img = img.cpu()\n",
        "\n",
        "      for b in range(len(ann['boxes'])):\n",
        "        ann['boxes'][b] = ann['boxes'][b]*sr_scale\n",
        "      ann['size'] = ann['size']*sr_scale\n",
        "\n",
        "\n",
        "    # Transforms if train\n",
        "    if self.do_transforms:\n",
        "      if self.op=='train':\n",
        "        # Color jitter\n",
        "        cj = torchvision.transforms.ColorJitter()\n",
        "        img = cj(img)\n",
        "\n",
        "        # Random horiz flip\n",
        "        if np.random.choice([True,False]):\n",
        "          img = torch.flip(img,[2])\n",
        "          for b in range(len(ann['boxes'])):\n",
        "            width = ann['size'][b][0].item()\n",
        "            xmin = ann['boxes'][b][0].item()\n",
        "            ymin = ann['boxes'][b][1].item()\n",
        "            xmax = ann['boxes'][b][2].item()\n",
        "            ymax = ann['boxes'][b][3].item()\n",
        "\n",
        "            ann['boxes'][b][0] = torch.Tensor([width - xmax])\n",
        "            ann['boxes'][b][1] = torch.Tensor([ymin])\n",
        "            ann['boxes'][b][2] = torch.Tensor([width - xmin])\n",
        "            ann['boxes'][b][3] = torch.Tensor([ymax])\n",
        "\n",
        "    # Return image and target\n",
        "    return img, ann"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORaTKx6raw-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d911364-b124-40b4-8696-b0d268b16029"
      },
      "source": [
        "# Instantiate data sets\n",
        "do_transforms=True\n",
        "sr=True\n",
        "\n",
        "trainData = MaskDataset(op='train/oversampling', sr=sr, do_transforms=do_transforms)\n",
        "print('Train:', len(trainData), trainData.counts)\n",
        "valData = MaskDataset(op='val', sr=sr, do_transforms=do_transforms)\n",
        "print('Val:', len(valData), valData.counts)\n",
        "testData = MaskDataset(op='test', sr=sr, do_transforms=do_transforms)\n",
        "print('Test:', len(testData), testData.counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 511 {'no_mask': 1511, 'masked': 1862, 'incorrect': 1413}\n",
            "Val: 171 {'no_mask': 117, 'masked': 614, 'incorrect': 30}\n",
            "Test: 171 {'no_mask': 139, 'masked': 756, 'incorrect': 27}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRFuZg7kR0ah"
      },
      "source": [
        "# Epoch loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeLZkghDR2Ye"
      },
      "source": [
        "def evalModel(model,dataLoader):\n",
        "  with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    for img, ann in dataLoader:\n",
        "      # Put on cuda if available\n",
        "      if torch.cuda.is_available():\n",
        "        img = list(i.cuda() for i in img)\n",
        "        ann = [{k:v.cuda() for k,v in a.items()} for a in ann]\n",
        "\n",
        "      # Get loss, add to loss container\n",
        "      loss = model(img,ann)\n",
        "      loss_sum = sum(l for l in loss.values())\n",
        "      total_loss += loss_sum.item()\n",
        "    return total_loss/len(dataLoader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFN0ZByTIUWW"
      },
      "source": [
        "# Training class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kp635VuXR1p"
      },
      "source": [
        "class TrainingModel():\n",
        "  def collate_fn(self,batch):\n",
        "      return tuple(zip(*batch))\n",
        "      \n",
        "  def __init__(self, model, batch_size, max_epochs, optimizer, \n",
        "               performTesting,print_every,saveFile):\n",
        "    self.model = model\n",
        "    self.batch_size = batch_size\n",
        "    self.max_epochs = max_epochs\n",
        "    self.optimizer = optimizer\n",
        "    self.performTesting = performTesting\n",
        "    self.print_every = print_every\n",
        "    self.saveFile = DATADIR[:DATADIR.rfind('/')] + '/Models/' + saveFile + '.pkl'\n",
        "    self.losses = {'train':[], 'val':[], 'test':[]}\n",
        "    self.minibatch_losses = []\n",
        "    self.epoch = 0\n",
        "\n",
        "    self.trainLoader = DataLoader(trainData, batch_size=self.batch_size, pin_memory=True, shuffle=True, collate_fn=self.collate_fn)\n",
        "    self.valLoader = DataLoader(valData, batch_size=self.batch_size, pin_memory=True, shuffle=True, collate_fn=self.collate_fn)\n",
        "    self.testLoader = DataLoader(testData, batch_size=self.batch_size, pin_memory=True, shuffle=True, collate_fn=self.collate_fn)\n",
        "\n",
        "  def save(self):\n",
        "    # Save\n",
        "    with open(self.saveFile, 'wb') as f:\n",
        "      pickle.dump(self, f)\n",
        "\n",
        "  def load(file):\n",
        "    # Load\n",
        "    with open(DATADIR[:DATADIR.rfind('/')] + '/Models/' + file + '.pkl', 'rb') as f:\n",
        "      tm = pickle.load(f)\n",
        "    return tm\n",
        "\n",
        "  def train(self):\n",
        "    # Get if cuda is available\n",
        "    cuda_available = torch.cuda.is_available()\n",
        "\n",
        "    # Put model on cuda\n",
        "    if cuda_available:\n",
        "      self.model = self.model.cuda()\n",
        "\n",
        "    # For each epoch\n",
        "    self.model.train()\n",
        "    print('starting training...')\n",
        "    for epoch in range(self.epoch,self.max_epochs,1):\n",
        "      self.epoch = epoch\n",
        "      # For each batch\n",
        "      batch_count = 0\n",
        "      for img, ann in self.trainLoader:\n",
        "        # Put on cuda if available\n",
        "        if cuda_available:\n",
        "          img = list(i.cuda() for i in img)\n",
        "          ann = [{k:v.cuda() for k,v in a.items()} for a in ann]\n",
        "\n",
        "        # Get loss, add to loss container for minibatch train loss\n",
        "        loss = self.model(img, ann)\n",
        "        loss_sum = sum(l for l in loss.values())\n",
        "        self.minibatch_losses.append(loss_sum.item())\n",
        "\n",
        "        # Update status\n",
        "        batch_count += 1\n",
        "        sys.stdout.write('\\rEpoch {} (Batch {}/{}) Loss: {:.8f}'.format(epoch,batch_count,len(self.trainLoader), loss_sum))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Backprop\n",
        "        self.optimizer.zero_grad()\n",
        "        loss_sum.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "      # Evaluate, print, and save periodically\n",
        "      with torch.no_grad():\n",
        "        if epoch % self.print_every == 0:\n",
        "          # Get val loss, append to container\n",
        "          self.losses['train'].append(np.sum(self.minibatch_losses[-len(self.trainLoader):])/len(self.trainLoader))\n",
        "          self.losses['val'].append(evalModel(self.model,self.valLoader))\n",
        "\n",
        "          # If testing, get testing loss and append to container\n",
        "          if self.performTesting:\n",
        "            self.losses['test'].append(evalModel(self.model,self.testLoader))\n",
        "          \n",
        "          # Print\n",
        "          if self.performTesting:\n",
        "            print('Epoch {}:\\tTrain loss: {:.4f}\\tVal loss: {:.4f}\\tTest loss: {:.4f}'.format(epoch, self.losses['train'][-1], self.losses['val'][-1], self.losses['test'][-1]))\n",
        "          else:\n",
        "            print('Epoch {}:\\tTrain loss: {:.4f}\\tVal loss: {:.4f}'.format(epoch, self.losses['train'][-1], self.losses['val'][-1]))\n",
        "\n",
        "          # Save\n",
        "          self.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGVNM3-4hnfn"
      },
      "source": [
        "## Perform training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQnaBRt2IV6T",
        "outputId": "472ae4b7-f12c-441f-cd20-788871405688"
      },
      "source": [
        "# Instantiate model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, rpn_nms_thresh=0.5, min_size=1600)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 3+1)\n",
        "\n",
        "fasterRCNN = TrainingModel(model=model,\n",
        "                        batch_size=4,\n",
        "                        max_epochs=50,\n",
        "                        optimizer=torch.optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
        "                                                   lr=1e-6,\n",
        "                                                   weight_decay=1e-4),\n",
        "                        performTesting=True,\n",
        "                        print_every=1,\n",
        "                        saveFile='final_os_sr')\n",
        "\n",
        "# Perform training\n",
        "fasterRCNN.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting training...\n",
            "Epoch 0 (Batch 128/128) Loss: 0.48878950Epoch 0:\tTrain loss: 1.0491\tVal loss: 0.4096\tTest loss: 0.4506\n",
            "Epoch 1 (Batch 128/128) Loss: 0.35971051Epoch 1:\tTrain loss: 0.5122\tVal loss: 0.3063\tTest loss: 0.3498\n",
            "Epoch 2 (Batch 128/128) Loss: 0.33409783Epoch 2:\tTrain loss: 0.4601\tVal loss: 0.2705\tTest loss: 0.3129\n",
            "Epoch 3 (Batch 128/128) Loss: 0.49742040Epoch 3:\tTrain loss: 0.4306\tVal loss: 0.2469\tTest loss: 0.2902\n",
            "Epoch 4 (Batch 128/128) Loss: 0.63708031Epoch 4:\tTrain loss: 0.4073\tVal loss: 0.2328\tTest loss: 0.2712\n",
            "Epoch 5 (Batch 128/128) Loss: 0.24576114Epoch 5:\tTrain loss: 0.3867\tVal loss: 0.2211\tTest loss: 0.2565\n",
            "Epoch 6 (Batch 128/128) Loss: 0.40656734Epoch 6:\tTrain loss: 0.3694\tVal loss: 0.2099\tTest loss: 0.2537\n",
            "Epoch 7 (Batch 128/128) Loss: 0.28973192Epoch 7:\tTrain loss: 0.3496\tVal loss: 0.2022\tTest loss: 0.2386\n",
            "Epoch 8 (Batch 128/128) Loss: 0.17637819Epoch 8:\tTrain loss: 0.3334\tVal loss: 0.1939\tTest loss: 0.2322\n",
            "Epoch 9 (Batch 128/128) Loss: 0.20392938Epoch 9:\tTrain loss: 0.3161\tVal loss: 0.1920\tTest loss: 0.2214\n",
            "Epoch 10 (Batch 128/128) Loss: 0.18074127Epoch 10:\tTrain loss: 0.3019\tVal loss: 0.1857\tTest loss: 0.2211\n",
            "Epoch 11 (Batch 128/128) Loss: 0.27367511Epoch 11:\tTrain loss: 0.2870\tVal loss: 0.1815\tTest loss: 0.2156\n",
            "Epoch 12 (Batch 128/128) Loss: 0.29272610Epoch 12:\tTrain loss: 0.2750\tVal loss: 0.1760\tTest loss: 0.2091\n",
            "Epoch 13 (Batch 128/128) Loss: 0.19418435Epoch 13:\tTrain loss: 0.2639\tVal loss: 0.1721\tTest loss: 0.2034\n",
            "Epoch 14 (Batch 128/128) Loss: 0.21174183Epoch 14:\tTrain loss: 0.2553\tVal loss: 0.1687\tTest loss: 0.2001\n",
            "Epoch 15 (Batch 128/128) Loss: 0.14599963Epoch 15:\tTrain loss: 0.2478\tVal loss: 0.1645\tTest loss: 0.1972\n",
            "Epoch 16 (Batch 128/128) Loss: 0.19255109Epoch 16:\tTrain loss: 0.2406\tVal loss: 0.1611\tTest loss: 0.1954\n",
            "Epoch 17 (Batch 128/128) Loss: 0.40052980Epoch 17:\tTrain loss: 0.2304\tVal loss: 0.1627\tTest loss: 0.1941\n",
            "Epoch 18 (Batch 128/128) Loss: 0.12436870Epoch 18:\tTrain loss: 0.2238\tVal loss: 0.1607\tTest loss: 0.1937\n",
            "Epoch 19 (Batch 128/128) Loss: 0.30732542Epoch 19:\tTrain loss: 0.2167\tVal loss: 0.1590\tTest loss: 0.1906\n",
            "Epoch 20 (Batch 128/128) Loss: 0.31359106Epoch 20:\tTrain loss: 0.2104\tVal loss: 0.1576\tTest loss: 0.1903\n",
            "Epoch 21 (Batch 128/128) Loss: 0.13836712Epoch 21:\tTrain loss: 0.2060\tVal loss: 0.1547\tTest loss: 0.1864\n",
            "Epoch 22 (Batch 128/128) Loss: 0.08450031Epoch 22:\tTrain loss: 0.2005\tVal loss: 0.1556\tTest loss: 0.1869\n",
            "Epoch 23 (Batch 128/128) Loss: 0.14138170Epoch 23:\tTrain loss: 0.1963\tVal loss: 0.1548\tTest loss: 0.1863\n",
            "Epoch 24 (Batch 128/128) Loss: 0.12472083Epoch 24:\tTrain loss: 0.1915\tVal loss: 0.1565\tTest loss: 0.1873\n",
            "Epoch 25 (Batch 128/128) Loss: 0.09268069Epoch 25:\tTrain loss: 0.1864\tVal loss: 0.1557\tTest loss: 0.1886\n",
            "Epoch 26 (Batch 128/128) Loss: 0.09500395Epoch 26:\tTrain loss: 0.1827\tVal loss: 0.1555\tTest loss: 0.1870\n",
            "Epoch 27 (Batch 128/128) Loss: 0.12412574Epoch 27:\tTrain loss: 0.1776\tVal loss: 0.1547\tTest loss: 0.1882\n",
            "Epoch 28 (Batch 128/128) Loss: 0.10747171Epoch 28:\tTrain loss: 0.1750\tVal loss: 0.1547\tTest loss: 0.1881\n",
            "Epoch 29 (Batch 128/128) Loss: 0.03576849Epoch 29:\tTrain loss: 0.1697\tVal loss: 0.1573\tTest loss: 0.1887\n",
            "Epoch 30 (Batch 128/128) Loss: 0.16490994Epoch 30:\tTrain loss: 0.1668\tVal loss: 0.1572\tTest loss: 0.1928\n",
            "Epoch 31 (Batch 128/128) Loss: 0.23612608Epoch 31:\tTrain loss: 0.1646\tVal loss: 0.1573\tTest loss: 0.1926\n",
            "Epoch 32 (Batch 128/128) Loss: 0.26980096Epoch 32:\tTrain loss: 0.1599\tVal loss: 0.1562\tTest loss: 0.1904\n",
            "Epoch 33 (Batch 128/128) Loss: 0.16732320Epoch 33:\tTrain loss: 0.1574\tVal loss: 0.1583\tTest loss: 0.1890\n",
            "Epoch 34 (Batch 128/128) Loss: 0.10186437Epoch 34:\tTrain loss: 0.1535\tVal loss: 0.1577\tTest loss: 0.1946\n",
            "Epoch 35 (Batch 128/128) Loss: 0.14592606Epoch 35:\tTrain loss: 0.1511\tVal loss: 0.1587\tTest loss: 0.1944\n",
            "Epoch 36 (Batch 128/128) Loss: 0.06468359Epoch 36:\tTrain loss: 0.1476\tVal loss: 0.1614\tTest loss: 0.1940\n",
            "Epoch 37 (Batch 128/128) Loss: 0.19727424Epoch 37:\tTrain loss: 0.1451\tVal loss: 0.1586\tTest loss: 0.1919\n",
            "Epoch 38 (Batch 128/128) Loss: 0.09459001Epoch 38:\tTrain loss: 0.1418\tVal loss: 0.1606\tTest loss: 0.1966\n",
            "Epoch 39 (Batch 128/128) Loss: 0.20199412Epoch 39:\tTrain loss: 0.1404\tVal loss: 0.1623\tTest loss: 0.1977\n",
            "Epoch 40 (Batch 128/128) Loss: 0.25812772Epoch 40:\tTrain loss: 0.1372\tVal loss: 0.1623\tTest loss: 0.1959\n",
            "Epoch 41 (Batch 128/128) Loss: 0.29418057Epoch 41:\tTrain loss: 0.1340\tVal loss: 0.1643\tTest loss: 0.2038\n",
            "Epoch 42 (Batch 128/128) Loss: 0.07404581Epoch 42:\tTrain loss: 0.1317\tVal loss: 0.1620\tTest loss: 0.1953\n",
            "Epoch 43 (Batch 128/128) Loss: 0.17776302Epoch 43:\tTrain loss: 0.1294\tVal loss: 0.1672\tTest loss: 0.2017\n",
            "Epoch 44 (Batch 128/128) Loss: 0.21654545Epoch 44:\tTrain loss: 0.1269\tVal loss: 0.1669\tTest loss: 0.2007\n",
            "Epoch 45 (Batch 128/128) Loss: 0.33378688Epoch 45:\tTrain loss: 0.1257\tVal loss: 0.1689\tTest loss: 0.2029\n",
            "Epoch 46 (Batch 128/128) Loss: 0.10636132Epoch 46:\tTrain loss: 0.1237\tVal loss: 0.1727\tTest loss: 0.2071\n",
            "Epoch 47 (Batch 128/128) Loss: 0.04874366Epoch 47:\tTrain loss: 0.1201\tVal loss: 0.1719\tTest loss: 0.2053\n",
            "Epoch 48 (Batch 128/128) Loss: 0.11189334Epoch 48:\tTrain loss: 0.1182\tVal loss: 0.1735\tTest loss: 0.2067\n",
            "Epoch 49 (Batch 128/128) Loss: 0.10178883Epoch 49:\tTrain loss: 0.1155\tVal loss: 0.1752\tTest loss: 0.2091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh5OLRASiAkX"
      },
      "source": [
        "### Resume training if interrupted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6MJPHiRc7k8"
      },
      "source": [
        "# fasterRCNN = TrainingModel.load('final_sr')\n",
        "# fasterRCNN.train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}